# 300m

Estimating that we will reach about 300-400m unique entities total.

```
$ du -hs --total *
265G    crossref
77G     datacite
170G    metha
413G    openalex
50G     pubmed
972G    total
```


To add: DBLP, DOAJ (maybe in metha)

## Acquisition

```
/var/data/schol/feeds
```

* [ ] automatic backfill after hours or days of interruptions
* [ ] find more compact ways to store, e.g. datacite
* [ ] systemd processes need to be adjusted to maybe read start date from disk

After a few weeks of regular updates we are at about 1.3TB compressed data
(est. 6-8TB uncompressed).

```
$ du -hs --total feeds/*
426G    feeds/crossref
123G    feeds/datacite
238G    feeds/metha
414G    feeds/openalex
51G     feeds/pubmed
1.3T    total
```

## Snapshots

Per raw data snapshots are nice, as a debugging help; but it's more effort to
implement snapshotting per source (as each source format is different). If we
first normalize, then we could run snapshotting on the normalized version; just
a single algorithm, but much more data to work through. Could also just
snapshot normalized data per source, which would not be too much.

```
/var/data/schol/snapshots
```

* [ ] compaction per source
* [ ] keep only most recent around

> Snapshot for crossref (about 1500 files):

```
martin@ia601101:/kubwa/schol $ TMPDIR=/kubwa/tmp time sk-crossref-snapshot -v -o /kubwa/schol/snapshots/sk-crossref-snapshot-2025-04-27.json.zst /kubwa/schol/feeds/crossref/crossref-feed-0-index-202*zst
```

> Openalex:

```
martin@ia601101:/kubwa/schol $ TMPDIR=/kubwa/tmp sk-snapshot -d /kubwa/schol/ -s openalex
...
 265M 14:59:39 [4.92k/s] [                                                                                                                                                                                   <=>                              ]
2025/04/27 15:51:29 snapshot created at: /kubwa/schol/snapshots/snapshot-openalex-2025-04-27.jsonl.zst
```

Likely slow due to parallel i/o from spinning disk; try sequential next.

```
martin@ia601101:/kubwa/schol $ TMPDIR=/kubwa/tmp sk-snapshot -d /kubwa/schol/ -s oai
```

Faster than expected, but also a bit smaller than expected; 110M records (may
be lag in harvest).

Ran the manual (on a scratch machine):

```
$ time find ~/.cache/metha/ -type f -name '*.gz' | parallel -j 32 'unpigz -c' | sk-oai-records | sk-oai-dctojsonl-stream | pv -l | zstd -c -T0 > /var/data/share/oaiscrape/2025-05-01-oaiscrape.jsonl.zst
real    1031m11.061s
user    2027m57.586s
sys     1795m43.168s
```

This results in 194GB compressed data in about 508M lines. Need to deduplicate,
first. Can iterate at about 36M lines/min.

Doing a basic dedup w/ sort:

```
$ time zstdcat -T0 2025-05-01-oaiscrape.jsonl.zst | LC_ALL=C sort -T /var/data/tmp/ -S 80% -u | zstd -c -T0 > 2025-05-01-oaiscrape-unique.jsonl.zst

real    39m19.667s
user    44m49.785s
sys     22m45.961s

$ zstdcat -T0 2025-05-01-oaiscrape-unique.jsonl.zst | wc -l # 7 minutes to iterate
300453726
```

> PUBMED:

Just concat the gzip; do compaction on the jsonline, later.

```
$ time sk-snapshot -d /kubwa/schol/ -s pubmed -v
2025/04/28 17:39:10 find /kubwa/schol/feeds/pubmed -type f -name "pubmed*.xml.gz" | parallel --block 10M --line-buffer -j 24 -I {} unpigz -c {} | zstd -c -T0 > /kubwa/schol/snapshots/snapshot-pubmed-2025-04-28.jsonl.zst
2025/04/28 19:10:41 snapshot created at: /kubwa/schol/snapshots/snapshot-pubmed-2025-04-28.jsonl.zst

real    91m30.579s
user    88m46.784s
sys     36m16.254s
```

> Datacite:

```
$ time sk-snapshot -w 4 -s datacite -d /kubwa/schol/ -T /fast/tmp
```

Total size (est).

```
snapshot-crossref-2025-04-27.jsonl.zst
168702386
snapshot-datacite-2025-05-01.jsonl.zst
43938471
snapshot-oai-2025-04-25.jsonl.zst
114584963
snapshot-openalex-2025-04-27.jsonl.zst
265686607
```

> 592912427 + missing datacite, missing oai, pubmed

Assume OAI: 200M, assume missing datacite: 15M, assume pubmed: 30M

> 592912427 + 90M (oai) + 15M + 30M = 727,912,427 records with duplicates

OAI:

```
$ zstdcat -T0 2025-05-01-oaiscrape-unique.jsonl.zst| pv -l | wc -l
 300M 0:07:04 [ 707k/s] [                                                                                                         <=>                                                                                                         ]
300453726

$ time zstdcat -T0 2025-05-01-oaiscrape-unique.jsonl.zst| wc -cl
300453726 555893499488 # 517GB

real    14m23.462s
user    11m13.865s
sys     5m23.256s
```

Update 06/2025 (no dedupe):

```
$ time zstdcat -T0 2025-06-06-oaiscrape-unique.jsonl.zst | pv | wc -l
 869GiB 0:21:44 [ 683MiB/s] [                    <=>                                                                                                                                                                                          ]
467379868

real    21m44.060s
user    14m15.304s
sys     8m58.414s
```

Note: metha single file per scrape window (e.g. monthly) does not scale for
creating snapshots, as those need to iterate over all files.

The OAI-PMH description token view:

* descriptions only amount to 39GB compressed; 217GB uncompressed; that's
  approx. 42573824917 tokens, so 42B tokens (cf. 242B in
https://www.institutionaldatainitiative.org/institutional-books)

```
$ zstdcat -T0 2025-05-01-oaiscrape-description.zst | pv | wc -wc
31,930,368,688 233889539833 # 217GB
```

WIP snapshot; 502M, w/ dups.

```
$ zstdcat -T0 2025-06-06-oaiscrape-unique.jsonl.zst | pv | wc -l
 928GiB 0:17:52 [ 886MiB/s] [                                                                                                               <=>                                                                                               ]
502142230
```




## Normalized ("pile")

Per source conversion to fatcat release schema, try to keep as much as possible
from original format. Any data fixes a source needs can be applied at this
stage.

```
u@h:/kubwa/schol $ time zstdcat -T0 snapshots/snapshot-crossref-2025-04-27.jsonl.zst | sk-convert -f crossref | pv -l | zstd -c -T0 > normalized/fatcat-crossref-2025-04-27.jsonl.zst

real    383m15.612s
user    465m1.652s
sys     98m19.071s

u@h:/kubwa/schol $ time zstdcat -T0 snapshots/snapshot-openalex-2025-04-27.jsonl.zst | sk-convert -f openalex | pv -l | zstd -c -T0 > normalized/fatcat-openalex-2025-04-27.jsonl.zst

real    560m48.170s
user    1422m59.637s
sys     207m34.374s

u@h:/kubwa/schol $ time zstdcat -T0 snapshots/snapshot-datacite-2025-05-01.jsonl.zst | sk-convert -f datacite | pv -l | zstd -c -T0 normalized/fatcat-datacite-2025-05-01.jsonl.zst
```

Datacite normalization; still some failed docs.


Fast XML processing had bugs, dropping pproc, skip snapshot and just convert
data from the individual files directly to fatcat schema.

```
$ cd normalized
$ time find ../feeds/pubmed/ -name '*pubmed*gz' | parallel 'zcat {} | sk-convert -B -f pubmed' | pv -l | zstd -c -T0 > fatcat-pubmed-04-28.jsonl.zst

real    48m1.200s
user    1000m43.674s
sys     45m36.022s
```



## Clustered

Extract a table of identifiers and various normalized title or title author
year strings and then do a second pass over that table.

```
/var/data/schol/catalog
```

This is a single file, containing all records clustered.



