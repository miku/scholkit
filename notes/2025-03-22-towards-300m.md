# 300m

Estimating that we will reach about 300-400m unique entities total.

```
$ du -hs --total *
265G    crossref
77G     datacite
170G    metha
413G    openalex
50G     pubmed
972G    total
```

To add: DBLP, DOAJ (maybe in metha)

## Acquisition

```
/var/data/schol/feeds
```

* [ ] automatic backfill after hours or days of interruptions
* [ ] find more compact ways to store, e.g. datacite
* [ ] systemd processes need to be adjusted to maybe read start date from disk

After a few weeks of regular updates we are at about 1.3TB compressed data
(est. 6-8TB uncompressed).

```
$ du -hs --total feeds/*
426G    feeds/crossref
123G    feeds/datacite
238G    feeds/metha
414G    feeds/openalex
51G     feeds/pubmed
1.3T    total
```

## Snapshots

Per raw data snapshots are nice, as a debugging help; but it's more effort to
implement snapshotting per source (as each source format is different). If we
first normalize, then we could run snapshotting on the normalized version; just
a single algorithm, but much more data to work through. Could also just
snapshot normalized data per source, which would not be too much.

```
/var/data/schol/snapshots
```

* [ ] compaction per source
* [ ] keep only most recent around

> Snapshot for crossref (about 1500 files):

```
martin@ia601101:/kubwa/schol $ TMPDIR=/kubwa/tmp time sk-crossref-snapshot -v -o /kubwa/schol/snapshots/sk-crossref-snapshot-2025-04-27.json.zst /kubwa/schol/feeds/crossref/crossref-feed-0-index-202*zst
```

> Openalex:

```
martin@ia601101:/kubwa/schol $ TMPDIR=/kubwa/tmp sk-snapshot -d /kubwa/schol/ -s openalex
...
 265M 14:59:39 [4.92k/s] [                                                                                                                                                                                   <=>                              ]
2025/04/27 15:51:29 snapshot created at: /kubwa/schol/snapshots/snapshot-openalex-2025-04-27.jsonl.zst
```

Likely slow due to parallel i/o from spinning disk; try sequential next.

> OAI:

```
martin@ia601101:/kubwa/schol $ TMPDIR=/kubwa/tmp sk-snapshot -d /kubwa/schol/ -s oai
```

Faster than expected, but also a bit smaller than expected; 110M records (may
be lag in harvest).

> PUBMED:

Just concat the gzip; do compaction on the jsonline, later.

```
$ time sk-snapshot -d /kubwa/schol/ -s pubmed -v
2025/04/28 17:39:10 find /kubwa/schol/feeds/pubmed -type f -name "pubmed*.xml.gz" | parallel --block 10M --line-buffer -j 24 -I {} unpigz -c {} | zstd -c -T0 > /kubwa/schol/snapshots/snapshot-pubmed-2025-04-28.jsonl.zst
2025/04/28 19:10:41 snapshot created at: /kubwa/schol/snapshots/snapshot-pubmed-2025-04-28.jsonl.zst

real    91m30.579s
user    88m46.784s
sys     36m16.254s
```

> Datacite:

```
$ time sk-snapshot -w 4 -s datacite -d /kubwa/schol/ -T /fast/tmp
```

Total size (est).

```
snapshot-crossref-2025-04-27.jsonl.zst
168702386
snapshot-datacite-2025-05-01.jsonl.zst
43938471
snapshot-oai-2025-04-25.jsonl.zst
114584963
snapshot-openalex-2025-04-27.jsonl.zst
265686607
```

> 592912427 + missing datacite, missing oai, pubmed



## Normalized ("pile")

Per source conversion to fatcat release schema, try to keep as much as possible
from original format. Any data fixes a source needs can be applied at this
stage.

```
martin@ia601101:/kubwa/schol $ time zstdcat -T0 snapshots/snapshot-crossref-2025-04-27.jsonl.zst | sk-convert -f crossref | pv -l | zstd -c -T0 > normalized/fatcat-crossref-2025-04-27.jsonl.zst

real    383m15.612s
user    465m1.652s
sys     98m19.071s

martin@ia601101:/kubwa/schol $ time zstdcat -T0 snapshots/snapshot-openalex-2025-04-27.jsonl.zst | sk-convert -f openalex | pv -l | zstd -c -T0 > normalized/fatcat-openalex-2025-04-27.jsonl.zst

real    560m48.170s
user    1422m59.637s
sys     207m34.374s
```





## Clustered

Extract a table of identifiers and various normalized title or title author
year strings and then do a second pass over that table.

```
/var/data/schol/catalog
```

This is a single file, containing all records clustered.



