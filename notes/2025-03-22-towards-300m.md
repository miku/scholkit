# 300m

Estimating that we will reach about 400m unique entities total.

## Acquisition

```
/var/data/schol/feeds
```

* [ ] automatic backfill after hours or days of interruptions
* [ ] find more compact ways to store, e.g. datacite
* [ ] systemd processes need to be adjusted to maybe read start date from disk

## Snapshots

Per raw data snapshots are nice, as a debugging help; but it's more effort to
implement snapshotting per source (as each source format is different). If we
first normalize, then we could run snapshotting on the normalized version; just
a single algorithm, but much more data to work through.

```
/var/data/schol/snapshots
```

* [ ] compaction per source
* [ ] keep only most recent around

## Normalized

Per source conversion to fatcat release schema, try to keep as much as possible
from original format. Any data fixes a source needs can be applied at this
stage.

```
/var/data/schol/normalized
```

## Clustered

Extract a table of identifiers and various normalized title or title author
year strings and then do a second pass over that table.

```
/var/data/schol/catalog
```

This is a single file, containing all records clustered.


